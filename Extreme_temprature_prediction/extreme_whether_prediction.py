# -*- coding: utf-8 -*-
"""Extreme_whether_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rOg6liqBSX5kijAffCQaNZPRsPGEKLgc
"""

import glob 

from google.colab import drive
drive.mount('/content/drive')

!pip install kaggle
!pip install split-folders

import os
os.environ['KAGGLE_CONFIG_DIR']="/content/drive/MyDrive/Kaggle"

!kaggle datasets download -d gauravduttakiit/extreme-weather-temperature-prediction

!ls

!unzip extreme-weather-temperature-prediction.zip

import pandas as pd
import numpy as np

df_train = pd.read_csv('/content/train_dataset.csv')

df_train.head()

df_train.shape

df_train.dropna(inplace=True)

df_train.isna().sum()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(32,12))
corr_matrix = df_train.corr()
sns.heatmap(corr_matrix,annot=True,cmap='coolwarm')
plt.show()

X = df_train.drop('Next_Tmax' ,axis='columns')
y = df_train['Next_Tmax']

from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import MinMaxScaler

X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.4,random_state=42,shuffle=True)

X_train.shape

y_train.shape

scalar= MinMaxScaler()

X_train_scaled= scalar.fit_transform(X_train)
X_test_scaled=scalar.fit_transform(X_test)

X_train_scaled

"""**Now we define our Neural Net And Predict our Tmax **"""

from keras.models import Sequential
from keras.layers import Dense, Dropout,BatchNormalization

# Define the model architecture
model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(21,)))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error',metrics=['accuracy'])

model.summary()

from keras.callbacks import ModelCheckpoint 
checkpoint =ModelCheckpoint('best_weights.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')

Batch_size=16
Epochs = 200
history = model.fit(X_train_scaled,y_train,batch_size=Batch_size, epochs=Epochs,validation_data=(X_test_scaled,y_test),callbacks=[checkpoint])

model.save_weights('best_weights.h5')
model.load_weights('best_weights.h5')

import matplotlib.pyplot as plt


# Plot training and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

test_loss  = model.evaluate(X_test_scaled, y_test)
print('Test loss:', test_loss)

X_test_scaled[0]

sample = X_test_scaled[4]


# reshape the sample to match the input shape of the model
sample = np.reshape(sample, (1, 21))

# make a prediction for the sample
prediction = model.predict(sample)

# print the prediction
print('Prediction:', prediction)

yhat

y_test